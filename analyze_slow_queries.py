"""
æ…¢æŸ¥è¯¢SQLè‡ªåŠ¨åˆ†æå·¥å…·ï¼ˆé›†æˆLangChainå’ŒDeepSeekæ™ºèƒ½ä¼˜åŒ–ï¼‰
1. ä»æ…¢æŸ¥è¯¢è¡¨è·å–æ…¢SQLï¼ˆæŸ¥è¯¢æ¬¡æ•°>10ï¼ŒæŸ¥è¯¢æ—¶é—´>10ï¼‰
2. æ ¹æ®SQLæ‰€åœ¨æ•°æ®åº“ï¼Œè‡ªåŠ¨åˆ†æè¯­å¥æ…¢çš„åŸå› 
3. ä½¿ç”¨LangChainå’ŒDeepSeek AIè¿›è¡Œæ™ºèƒ½ä¼˜åŒ–å»ºè®®
"""

import pymysql
import json
import requests
from typing import List, Dict, Optional, Any, Tuple
from mysql_slow_query_optimizer import MySQLSlowQueryOptimizer
import re
import os
import logging
from datetime import datetime, timedelta

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
CORE_ISSUE_PATTERN = re.compile(r'[*]{2}\s*æ ¸å¿ƒé—®é¢˜\s*[*]{2}\s*[:ï¼š]\s*(.*?)\s*(?:[*]{2}|\n)', re.DOTALL)
CLEAN_STARS_PATTERN = re.compile(r'[*]{2}')
SECTION_PATTERN = re.compile(r'(æ ¸å¿ƒé—®é¢˜|æœ€ä¼˜ä¼˜åŒ–æ–¹æ¡ˆ|æœ€ä¼˜æ–¹æ¡ˆ|ä¼˜åŒ–æ–¹æ¡ˆ|é¢„æœŸæ•ˆæœ|æ€§èƒ½æå‡|æ•ˆæœ|ä¸»è¦é—®é¢˜|é—®é¢˜)\s*[:ï¼š]\s*(.*?)(?=\n\s*(æ ¸å¿ƒé—®é¢˜|æœ€ä¼˜ä¼˜åŒ–æ–¹æ¡ˆ|æœ€ä¼˜æ–¹æ¡ˆ|ä¼˜åŒ–æ–¹æ¡ˆ|é¢„æœŸæ•ˆæœ|æ€§èƒ½æå‡|æ•ˆæœ|ä¸»è¦é—®é¢˜|é—®é¢˜)\s*[:ï¼š]|\Z)', re.S)
VALID_TABLE_NAME_PATTERN = re.compile(r'^[a-zA-Z0-9_.]+$')
VALID_DB_NAME_PATTERN = re.compile(r'^[a-zA-Z0-9_]+$')

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('SlowQueryAnalyzer')

# è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†datetimeå¯¹è±¡
class DateTimeEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, timedelta):
            return str(obj)
        return super().default(obj)

# æ™ºèƒ½æ•°æ®åº“åè¯†åˆ«å‡½æ•°
def extract_db_table_from_sql(sql_content: str) -> Tuple[Optional[str], Optional[str]]:
    """ä»SQLè¯­å¥ä¸­æå–æ•°æ®åº“åå’Œè¡¨å"""
    if not sql_content:
        return None, None
        
    sql_clean = sql_content.strip()
    sql_upper = sql_clean.upper()
    
    # æ•°æ®åº“.è¡¨å æ¨¡å¼
    db_table_patterns = [
        r'(?:FROM|JOIN)\s+`?(\w+)`?\.`?(\w+)`?',
        r'INSERT\s+INTO\s+`?(\w+)`?\.`?(\w+)`?',
        r'UPDATE\s+`?(\w+)`?\.`?(\w+)`?',
        r'DELETE\s+FROM\s+`?(\w+)`?\.`?(\w+)`?',
    ]
    
    # å…ˆå°è¯•æå–æ•°æ®åº“.è¡¨åæ ¼å¼
    for pattern in db_table_patterns:
        matches = re.findall(pattern, sql_upper, re.IGNORECASE)
        if matches:
            db_name, table_name = matches[0]
            return db_name.lower(), table_name.lower()
    
    # åªæå–è¡¨åï¼ˆç®€å•è¡¨åï¼‰
    table_patterns = [
        r'(?:FROM|JOIN)\s+`?(\w+)`?(?!\.)',
        r'INSERT\s+INTO\s+`?(\w+)`?(?!\.)',
        r'UPDATE\s+`?(\w+)`?(?!\.)',
        r'DELETE\s+FROM\s+`?(\w+)`?(?!\.)',
    ]
    
    for pattern in table_patterns:
        matches = re.findall(pattern, sql_upper, re.IGNORECASE)
        if matches:
            return None, matches[0].lower()
    
    return None, None

def find_database_for_table(connection, table_name: str) -> Optional[str]:
    """åœ¨æ‰€æœ‰æ•°æ®åº“ä¸­æŸ¥æ‰¾åŒ…å«æŒ‡å®šè¡¨çš„æ•°æ®åº“"""
    
    # è°ƒè¯•ï¼šæ£€æŸ¥ä¼ å…¥çš„connectionå‚æ•°
    logger.debug(f"find_database_for_table - æ¥æ”¶åˆ°çš„connectionç±»å‹: {type(connection)}")
    if connection is not None:
        logger.debug(f"find_database_for_table - connectionæœ‰cursorå±æ€§: {hasattr(connection, 'cursor')}")
    
    if not table_name or not connection:
        return None
    
    # éªŒè¯connectionå¯¹è±¡
    if not hasattr(connection, 'cursor'):
        logger.error(f"find_database_for_table - connectionå¯¹è±¡æ— æ•ˆ! ç±»å‹: {type(connection)}, ç¼ºå°‘cursoræ–¹æ³•")
        return None
        
    # æ’é™¤ç³»ç»Ÿæ•°æ®åº“
    excluded_databases = {
        'information_schema', 'mysql', 'performance_schema', 'sys',
        'c2c_db', 'test', 'tmp'
    }
    
    try:
        with connection.cursor() as cursor:
            # è·å–æ‰€æœ‰æ•°æ®åº“
            cursor.execute("SHOW DATABASES")
            databases = [row['Database'] for row in cursor.fetchall()]
            
            # è¿‡æ»¤æ‰ç³»ç»Ÿæ•°æ®åº“
            candidate_databases = [db for db in databases if db not in excluded_databases]

            
            # åœ¨æ¯ä¸ªå€™é€‰æ•°æ®åº“ä¸­æŸ¥æ‰¾è¡¨
            for db in candidate_databases:
                try:
                    cursor.execute(f"USE `{db}`")
                    cursor.execute(f"SHOW TABLES LIKE '{table_name}'")
                    result = cursor.fetchone()
                    
                    if result:

                        return db
                        
                except Exception as e:
                    logger.debug(f"åœ¨æ•°æ®åº“ '{db}' ä¸­æŸ¥æ‰¾è¡¨å¤±è´¥: {e}")
                    continue
            
            logger.debug(f"åœ¨æ‰€æœ‰å€™é€‰æ•°æ®åº“ä¸­å‡æœªæ‰¾åˆ°è¡¨ '{table_name}'")
            return None
            
    except Exception as e:
        logger.debug(f"æŸ¥æ‰¾æ•°æ®åº“å¤±è´¥: {e}")
        return None

def get_intelligent_db_name(sql_content: str, table_name: Optional[str] = None, 
                          connection=None, hostname: str = "") -> str:
    """æ™ºèƒ½è¯†åˆ«æ•°æ®åº“å"""
    
    # è°ƒè¯•ï¼šæ£€æŸ¥connectionå‚æ•°
    logger.debug(f"get_intelligent_db_name - connectionç±»å‹: {type(connection)}")
    logger.debug(f"get_intelligent_db_name - connectionå€¼: {connection}")
    if connection is not None:
        logger.debug(f"get_intelligent_db_name - connectionå±æ€§: {dir(connection)[:5]}...")
    
    # 1. ä»SQLè¯­å¥ä¸­æå–æ•°æ®åº“å
    db_from_sql, table_from_sql = extract_db_table_from_sql(sql_content)
    
    if db_from_sql:
        logger.debug(f"ä»SQLè¯­å¥æå–åˆ°æ•°æ®åº“å: {db_from_sql}")
        return db_from_sql
    
    # 2. å¦‚æœæå–åˆ°è¡¨åä½†æ²¡æœ‰æ•°æ®åº“åï¼Œå°è¯•æŸ¥æ‰¾æ•°æ®åº“
    table_to_find = table_name or table_from_sql
    
    if table_to_find and connection:
        logger.debug(f"ä»SQLæå–åˆ°è¡¨å: {table_to_find}ï¼Œæ­£åœ¨æŸ¥æ‰¾æ•°æ®åº“...")
        db_found = find_database_for_table(connection, table_to_find)
        if db_found:
            logger.debug(f"æ‰¾åˆ°æ•°æ®åº“: {db_found}")
            return db_found
    
    # 3. æ™ºèƒ½é»˜è®¤é€»è¾‘
    if table_to_find:
        # åŸºäºè¡¨åçš„æ™ºèƒ½é»˜è®¤
        if table_to_find == 't':
            return 'db'  # åŸºäºä¹‹å‰çš„è°ƒè¯•ï¼Œè¡¨'t'åœ¨æ•°æ®åº“'db'ä¸­
        elif table_to_find in ['user', 'users']:
            return 'db'
        elif table_to_find in ['order', 'orders', 'product', 'products']:
            return 'db'
        else:
            return 'db'  # é€šç”¨é»˜è®¤
    
    # 4. æœ€åçš„fallback
    return 'db'  # æœ€å®‰å…¨çš„é»˜è®¤

# å°è¯•å¯¼å…¥LangChainç›¸å…³æ¨¡å—
try:
    from langchain.prompts import PromptTemplate
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False


class SlowQueryAnalyzer:
    """æ…¢æŸ¥è¯¢åˆ†æå™¨ï¼ˆé›†æˆLangChainå’ŒDeepSeekæ™ºèƒ½ä¼˜åŒ–ï¼‰"""
    
    def __init__(self, 
                 slow_query_db_host: str = None,
                 slow_query_db_user: str = None,
                 slow_query_db_password: str = None,
                 slow_query_db_port: int = None,
                 slow_query_db_name: str = None,
                 slow_query_table: str = None,
                 deepseek_api_key: str = None):
        """
        åˆå§‹åŒ–æ…¢æŸ¥è¯¢åˆ†æå™¨
        
        Args:
            slow_query_db_host: æ…¢æŸ¥è¯¢è¡¨æ‰€åœ¨æ•°æ®åº“IP
            slow_query_db_user: æ…¢æŸ¥è¯¢è¡¨æ‰€åœ¨æ•°æ®åº“ç”¨æˆ·å
            slow_query_db_password: æ…¢æŸ¥è¯¢è¡¨æ‰€åœ¨æ•°æ®åº“å¯†ç 
            slow_query_db_port: æ…¢æŸ¥è¯¢è¡¨æ‰€åœ¨æ•°æ®åº“ç«¯å£
            slow_query_db_name: æ…¢æŸ¥è¯¢è¡¨æ‰€åœ¨æ•°æ®åº“åï¼ˆå¦‚æœè¡¨ä¸åœ¨é»˜è®¤æ•°æ®åº“ï¼‰
            slow_query_table: æ…¢æŸ¥è¯¢è¡¨å
            deepseek_api_key: DeepSeek APIå¯†é’¥
        """
        # ä»ç¯å¢ƒå˜é‡å®‰å…¨è¯»å–é…ç½®ï¼Œç§»é™¤ç¡¬ç¼–ç é»˜è®¤å€¼
        self.slow_query_db_host = slow_query_db_host or os.environ.get('SLOW_QUERY_DB_HOST')
        self.slow_query_db_user = slow_query_db_user or os.environ.get('SLOW_QUERY_DB_USER')
        self.slow_query_db_password = slow_query_db_password or os.environ.get('SLOW_QUERY_DB_PASSWORD')
        self.slow_query_db_port = slow_query_db_port or int(os.environ.get('SLOW_QUERY_DB_PORT', '3306'))
        self.slow_query_db_name = slow_query_db_name or os.environ.get('SLOW_QUERY_DB_NAME')
        self.slow_query_table = slow_query_table or os.environ.get('SLOW_QUERY_TABLE', 'slow')
        
        # éªŒè¯å¿…éœ€çš„æ•°æ®åº“é…ç½®
        if not all([self.slow_query_db_host, self.slow_query_db_user, self.slow_query_db_password]):
            raise ValueError("æ•°æ®åº“è¿æ¥é…ç½®ä¸å®Œæ•´ï¼Œè¯·è®¾ç½®å¿…éœ€çš„ç¯å¢ƒå˜é‡")
        logger.info(f"æ…¢æŸ¥è¯¢è¡¨åå·²è®¾ç½®ä¸º: {self.slow_query_table}")
        self.deepseek_api_key = deepseek_api_key or os.environ.get('DEEPSEEK_API_KEY', 'sk-0745b17c589b4074a2f9d9e88f83bb76')
        
        # åˆå§‹åŒ–LangChain PromptTemplateï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if LANGCHAIN_AVAILABLE:
            self._init_langchain_prompts()
        
    def get_slow_queries(self, min_execute_cnt: int = 0, min_query_time: float = 0.0, month_offset: int = 1) -> List[Dict]:
        """
        ä»æ…¢æŸ¥è¯¢è¡¨è·å–æ…¢SQL
        
        Args:
            min_execute_cnt: æœ€å°æŸ¥è¯¢æ¬¡æ•°
            min_query_time: æœ€å°æŸ¥è¯¢æ—¶é—´ï¼ˆç§’ï¼‰
            month_offset: æœˆä»½åç§»é‡ï¼ˆ1è¡¨ç¤ºä¸Šä¸ªæœˆï¼Œ2è¡¨ç¤ºä¸Šä¸Šä¸ªæœˆï¼‰
            
        Returns:
            æ…¢æŸ¥è¯¢SQLåˆ—è¡¨
        """
        slow_queries = []
        
        try:
            # è®¡ç®—ç›®æ ‡æœˆä»½çš„æ—¥æœŸèŒƒå›´
            today = datetime.now()
            # è·å–å½“å‰æœˆçš„ç¬¬ä¸€å¤©
            first_day_of_current_month = today.replace(day=1)
            
            # æ ¹æ®åç§»é‡è®¡ç®—ç›®æ ‡æœˆä»½
            # åˆå§‹åŒ–ä¸Šä¸ªæœˆçš„ç¬¬ä¸€å¤©å’Œæœ€åä¸€å¤©
            if month_offset > 0:
                # å…ˆè®¡ç®—ä¸Šä¸ªæœˆçš„ç¬¬ä¸€å¤©
                # ä¾‹å¦‚ï¼šå¦‚æœå½“å‰æ˜¯1æœˆï¼Œå‡1å¤©ä¼šå¾—åˆ°12æœˆçš„æŸä¸€å¤©
                last_day_of_previous_month = first_day_of_current_month - timedelta(days=1)
                first_day_of_previous_month = last_day_of_previous_month.replace(day=1)
                
                # å¦‚æœåç§»é‡å¤§äº1ï¼Œç»§ç»­å‘å‰è®¡ç®—
                for _ in range(month_offset - 1):
                    last_day_of_previous_month = first_day_of_previous_month - timedelta(days=1)
                    first_day_of_previous_month = last_day_of_previous_month.replace(day=1)
            else:
                # å¦‚æœåç§»é‡ä¸º0ï¼Œä½¿ç”¨å½“å‰æœˆ
                first_day_of_previous_month = first_day_of_current_month
                # è®¡ç®—å½“å‰æœˆçš„æœ€åä¸€å¤©ï¼ˆä¸‹ä¸ªæœˆç¬¬ä¸€å¤©å‡1å¤©ï¼‰
                if today.month == 12:
                    last_day_of_previous_month = datetime(today.year + 1, 1, 1) - timedelta(days=1)
                else:
                    last_day_of_previous_month = datetime(today.year, today.month + 1, 1) - timedelta(days=1)
            
            # æ ¼å¼åŒ–ä¸ºYYYY-MM-DDæ ¼å¼
            start_date = first_day_of_previous_month.strftime('%Y-%m-%d')
            end_date = last_day_of_previous_month.strftime('%Y-%m-%d')
            
            logger.info(f"æ­£åœ¨è¿æ¥åˆ°æ…¢æŸ¥è¯¢æ•°æ®åº“: {self.slow_query_db_host}:{self.slow_query_db_port}")
            
            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è¿æ¥æ•°æ®åº“ï¼Œç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
            with pymysql.connect(
                host=self.slow_query_db_host,
                port=self.slow_query_db_port,
                user=self.slow_query_db_user,
                password=self.slow_query_db_password,
                database=self.slow_query_db_name,  # å¦‚æœæŒ‡å®šäº†æ•°æ®åº“ååˆ™ä½¿ç”¨
                charset='utf8mb4',
                connect_timeout=5,
                read_timeout=10
            ) as connection:
                logger.info(f"æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“")
                logger.debug(f"è¿æ¥å¯¹è±¡ç±»å‹: {type(connection)}")
                logger.debug(f"è¿æ¥å¯¹è±¡å±æ€§: {dir(connection)[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ªå±æ€§
                
                # éªŒè¯è¿æ¥å¯¹è±¡æ˜¯å¦æœ‰cursoræ–¹æ³•
                if not hasattr(connection, 'cursor'):
                    logger.error(f"è¿æ¥å¯¹è±¡ç¼ºå°‘cursoræ–¹æ³•! ç±»å‹: {type(connection)}")
                    logger.error(f"è¿æ¥å¯¹è±¡æ‰€æœ‰å±æ€§: {dir(connection)}")
                    raise AttributeError(f"è¿æ¥å¯¹è±¡ç±»å‹ {type(connection)} æ²¡æœ‰cursoræ–¹æ³•")
                
                # é¢å¤–éªŒè¯ï¼šç¡®ä¿è¿™æ˜¯PyMySQLè¿æ¥å¯¹è±¡
                try:
                    # ä½¿ç”¨å…¨å±€pymysqlæ¨¡å—è¿›è¡Œç±»å‹æ£€æŸ¥
                    if not isinstance(connection, pymysql.connections.Connection):
                        logger.error(f"è¿æ¥å¯¹è±¡ä¸æ˜¯PyMySQL Connectionç±»å‹! å®é™…ç±»å‹: {type(connection)}")
                        raise TypeError(f"æœŸæœ›PyMySQL Connectionï¼Œä½†å¾—åˆ° {type(connection)}")
                except (ImportError, AttributeError):
                    # å¦‚æœæ— æ³•è®¿é—®pymysql.connectionsï¼Œè·³è¿‡ç±»å‹æ£€æŸ¥
                    logger.debug("æ— æ³•è®¿é—®pymysql.connectionsï¼Œè·³è¿‡è¿æ¥ç±»å‹éªŒè¯")
                
                logger.debug(f"è¿æ¥å¯¹è±¡éªŒè¯é€šè¿‡ï¼Œå‡†å¤‡åˆ›å»ºæ¸¸æ ‡")
                with connection.cursor(pymysql.cursors.DictCursor) as cursor:
                    # æ„å»ºæŸ¥è¯¢SQLï¼Œå¦‚æœæŒ‡å®šäº†æ•°æ®åº“ååˆ™ä½¿ç”¨ database.table æ ¼å¼
                    # éªŒè¯è¡¨åå®‰å…¨æ€§
                    logger.info(f"å½“å‰ä½¿ç”¨çš„æ…¢æŸ¥è¯¢è¡¨å: {self.slow_query_table}")
                    if not re.match(r'^[a-zA-Z0-9_.]+$', self.slow_query_table):
                        raise ValueError(f"è¡¨ååŒ…å«éæ³•å­—ç¬¦: {self.slow_query_table}")
                    
                    if self.slow_query_db_name:
                        # éªŒè¯æ•°æ®åº“åå®‰å…¨æ€§
                        if not re.match(r'^[a-zA-Z0-9_]+$', self.slow_query_db_name):
                            raise ValueError(f"æ•°æ®åº“ååŒ…å«éæ³•å­—ç¬¦: {self.slow_query_db_name}")
                        table_ref = f"`{self.slow_query_db_name}`.`{self.slow_query_table}`"
                    else:
                        table_ref = f"`{self.slow_query_table}`"
                    
                    # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆå‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥ï¼‰
                    cursor.execute("SHOW TABLES LIKE %s", (self.slow_query_table,))
                    table_exists = cursor.fetchone() is not None
                    
                    if not table_exists:
                        logger.info(f"è¡¨ {self.slow_query_table} ä¸å­˜åœ¨")
                        # è¡¨ä¸å­˜åœ¨æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œè®©ä¸Šå±‚é€»è¾‘å¤„ç†
                        return slow_queries
                    
                    # å°è¯•ä¸åŒçš„æŸ¥è¯¢æ–¹å¼ï¼Œä½¿ç”¨try-exceptç»“æ„
                    query_templates = [
                        # æ¨¡æ¿1ï¼šä¿®æ”¹ä¸ºåŒ¹é…çœŸå®è¡¨ç»“æ„çš„æŸ¥è¯¢
                        """
                        SELECT
                        checksum,
                        sample as sql_content,
                        ts_cnt as execute_cnt,
                        query_time_max as query_time,
                        hostname_max
                        FROM {}
                        where ts_min >= %s AND ts_min <= %s
                        HAVING execute_cnt > %s and query_time > %s
                        ORDER BY execute_cnt, hostname_max
                        """.format(table_ref),
                    ]
                    
                    for i, template in enumerate(query_templates, 1):
                        print(f"æŸ¥è¯¢æ¨¡æ¿ {i}:")
                        print(template)
                        print("-----------------------------------------")
                    
                    results = []
                    query_success = False
                    
                    # å°è¯•æ¯ç§æŸ¥è¯¢æ¨¡æ¿
                    for i, template in enumerate(query_templates, 1):
                        try:
                            logger.info(f"å°è¯•æŸ¥è¯¢æ¨¡æ¿ {i}")
                            # è·å–æŸ¥è¯¢å‚æ•°
                            params = (start_date, end_date, min_execute_cnt, min_query_time) if i == 1 else (start_date, end_date)
                            # æ‰§è¡ŒæŸ¥è¯¢
                            cursor.execute(template, params)
                            # æ‰“å°å®Œæ•´çš„æŸ¥è¯¢SQLè¯­å¥ï¼ˆåŒ…å«å‚æ•°å€¼ï¼‰
                            print(f"\nğŸ” æ‰§è¡Œçš„å®Œæ•´SQLè¯­å¥ï¼š")
                            print("-----------------------------------------")
                            # æ„å»ºå¸¦å‚æ•°çš„SQLè¯­å¥ç”¨äºæ‰“å°ï¼ˆæ³¨æ„ï¼šå®é™…æ‰§è¡Œä»ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼‰
                            if hasattr(cursor, '_last_executed'):
                                print(cursor._last_executed.decode('utf-8') if isinstance(cursor._last_executed, bytes) else str(cursor._last_executed))
                            else:
                                # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼æ˜¾ç¤ºå‚æ•°ï¼Œé¿å…æ ¼å¼å­—ç¬¦ä¸²å†²çª
                                display_sql = template
                                for param in params:
                                    # å°†æ¯ä¸ª%sæ›¿æ¢ä¸ºå‚æ•°çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œé¿å…æ ¼å¼å†²çª
                                    display_sql = display_sql.replace('%s', repr(param), 1)
                                print(display_sql)
                            print("-----------------------------------------")
                            
                            # è·å–æŸ¥è¯¢ç»“æœ
                            results = cursor.fetchall()
                            logger.info(f"æŸ¥è¯¢æ¨¡æ¿ {i} æˆåŠŸï¼Œè·å–åˆ° {len(results)} æ¡è®°å½•")
                            query_success = True
                            break
                            
                        except Exception as e:
                            # å¤„ç†æ•°æ®åº“ç›¸å…³é”™è¯¯
                            error_code = getattr(e, 'args', [0])[0] if hasattr(e, 'args') else 0
                            error_msg = str(e)
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯MySQLé”™è¯¯
                            if 'MySQL' in str(type(e)) or error_code in [1054, 1146]:
                                logger.warning(f"æŸ¥è¯¢æ¨¡æ¿ {i} å¤±è´¥ (é”™è¯¯ç : {error_code}): {error_msg}")
                                
                                if error_code == 1054:  # Unknown column
                                    logger.info("æ£€æµ‹åˆ°æœªçŸ¥åˆ—é”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæŸ¥è¯¢æ¨¡æ¿")
                                    continue
                                elif error_code == 1146:  # Table doesn't exist
                                    logger.warning(f"è¡¨ä¸å­˜åœ¨: {table_ref}")
                                    return slow_queries
                                else:
                                    logger.error(f"æŸ¥è¯¢å¤±è´¥: {error_msg}")
                                    raise
                            else:
                                # éMySQLé”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                                raise
                    
                    if not query_success:
                        logger.error("æ‰€æœ‰æŸ¥è¯¢æ¨¡æ¿å‡å¤±è´¥")
                        return slow_queries
                    
                    logger.info(f"ä»æ…¢æŸ¥è¯¢è¡¨è·å–åˆ° {len(results)} æ¡æ…¢æŸ¥è¯¢è®°å½•")
                    
                    # å¤„ç†æŸ¥è¯¢ç»“æœ
                    for i, row in enumerate(results):
                        try:
                            # è°ƒè¯•ï¼šè·Ÿè¸ªconnectionå˜é‡çŠ¶æ€
                            logger.debug(f"å¤„ç†ç¬¬{i}è¡Œæ•°æ® - connectionç±»å‹: {type(connection)}")
                            if connection is not None and not hasattr(connection, 'cursor'):
                                logger.error(f"å¤„ç†ç¬¬{i}è¡Œæ—¶connectionå¯¹è±¡å¼‚å¸¸! ç±»å‹: {type(connection)}, å±æ€§: {dir(connection)[:5]}")
                                raise AttributeError(f"connectionå¯¹è±¡åœ¨ç¬¬{i}è¡Œå¤„ç†æ—¶å˜ä¸º {type(connection)} ç±»å‹ï¼Œç¼ºå°‘cursoræ–¹æ³•")
                            
                            sql_content = row.get('sql_content', '') or row.get('sample', '')
                            if not sql_content:
                                logger.warning("SQLå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                                continue
                            
                            # æå–è¡¨å
                            table_name = self.extract_table_name(sql_content)
                            if not table_name:
                                logger.warning(f"æ— æ³•ä»SQLä¸­æå–è¡¨å: {sql_content[:100]}...")
                                continue
                            
                            # è·å–ä¸»æœºä¿¡æ¯
                            hostname_max = row.get('hostname_max', '') or row.get('host', '') or 'localhost'
                            
                            # è·å–åŸå§‹æ•°æ®åº“å
                            original_db_name = row.get('db_name', '') or row.get('database', '')
                            
                            # è°ƒè¯•ï¼šæ£€æŸ¥è°ƒç”¨å‰çš„connectionå˜é‡
                            logger.debug(f"è°ƒç”¨get_intelligent_db_nameå‰ - connectionç±»å‹: {type(connection)}")
                            logger.debug(f"è°ƒç”¨get_intelligent_db_nameå‰ - connectionå€¼: {connection}")
                            if connection is not None:
                                logger.debug(f"è°ƒç”¨get_intelligent_db_nameå‰ - æœ‰cursorå±æ€§: {hasattr(connection, 'cursor')}")
                            
                            # ä½¿ç”¨æ™ºèƒ½æ•°æ®åº“åè¯†åˆ« - ä½¿ç”¨get_intelligent_db_nameå‡½æ•°
                            intelligent_db_name = get_intelligent_db_name(sql_content, table_name, connection, hostname_max)
                            
                            # å¦‚æœæ™ºèƒ½è¯†åˆ«æˆåŠŸä¸”ä¸åŸå§‹æ•°æ®åº“åä¸åŒï¼Œä½¿ç”¨æ™ºèƒ½è¯†åˆ«çš„ç»“æœ
                            if intelligent_db_name and intelligent_db_name != original_db_name:
                                logger.info(f"æ•°æ®åº“åæ™ºèƒ½è¯†åˆ«: {original_db_name} -> {intelligent_db_name} (SQL: {sql_content[:50]}...)")
                                final_db_name = intelligent_db_name
                            else:
                                final_db_name = original_db_name or intelligent_db_name or 'db'
                            
                            slow_query = {
                                'ip': row.get('ip', hostname_max),
                                'hostname_max': hostname_max,
                                'db_name': final_db_name,
                                'sql_content': sql_content,
                                'execute_cnt': row.get('execute_cnt', 0),
                                'query_time': row.get('query_time', 0.0),
                                'table_name': table_name,
                                'original_db_name': original_db_name,
                                'intelligent_db_name': intelligent_db_name
                            }
                            
                            slow_queries.append(slow_query)
                            
                        except Exception as e:
                            logger.error(f"å¤„ç†æ…¢æŸ¥è¯¢è®°å½•å¤±è´¥: {e}")
                            continue
                    
                    logger.info(f"æˆåŠŸå¤„ç† {len(slow_queries)} æ¡æ…¢æŸ¥è¯¢è®°å½•")
                    return slow_queries
                    
        except Exception as e:
            # å¤„ç†æ•°æ®åº“ç›¸å…³é”™è¯¯
            error_code = getattr(e, 'args', [0])[0] if hasattr(e, 'args') else 0
            error_msg = str(e)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯MySQLé”™è¯¯
            if 'MySQL' in str(type(e)) or error_code in [1046]:
                logger.error(f"æ•°æ®åº“é”™è¯¯ (é”™è¯¯ç : {error_code}): {error_msg}", exc_info=True)
                print(f"âœ— æ•°æ®åº“è¿æ¥æˆ–æŸ¥è¯¢å¤±è´¥: {error_msg}")
                
                if error_code == 1046:  # No database selected
                    print(f"âš  é”™è¯¯ï¼šæœªé€‰æ‹©æ•°æ®åº“ï¼Œè¯·åœ¨åˆ›å»ºåˆ†æå™¨æ—¶è®¾ç½® slow_query_db_name å‚æ•°")
            else:
                # éMySQLé”™è¯¯ï¼Œè®°å½•åé‡æ–°æŠ›å‡º
                logger.error(f"è·å–æ…¢æŸ¥è¯¢SQLå¤±è´¥: {str(e)}", exc_info=True)
                print(f"âœ— è·å–æ…¢æŸ¥è¯¢SQLå¤±è´¥: {e}")
                raise
        except ValueError as e:
            logger.error(f"å‚æ•°éªŒè¯é”™è¯¯: {str(e)}")
            print(f"âœ— å‚æ•°éªŒè¯å¤±è´¥: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"è·å–æ…¢æŸ¥è¯¢SQLå¤±è´¥: {str(e)}", exc_info=True)
            print(f"âœ— è·å–æ…¢æŸ¥è¯¢SQLå¤±è´¥: {e}")
        
        return slow_queries

    def extract_table_name(self, sql: str) -> Optional[str]:
        """
        ä»SQLè¯­å¥ä¸­æå–è¡¨åï¼ˆä¼˜å…ˆæå–ç¬¬ä¸€ä¸ªä¸»è¡¨ï¼‰
        
        Args:
            sql: SQLè¯­å¥
            
        Returns:
            è¡¨åï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        sql_clean = sql.strip()
        sql_upper = sql_clean.upper()
        
        # æå–FROMåçš„è¡¨åï¼ˆæœ€å¸¸ç”¨ï¼‰
        from_patterns = [
            r'FROM\s+`?([a-zA-Z0-9_]+)`?\s',  # FROM `table` æˆ– FROM table
            r'FROM\s+([a-zA-Z0-9_]+)\s',      # FROM table
            r'FROM\s+`?([a-zA-Z0-9_]+)`?$',   # FROM tableç»“å°¾
        ]
        
        for pattern in from_patterns:
            match = re.search(pattern, sql_upper, re.IGNORECASE)
            if match:
                table = match.group(1)
                # æ’é™¤ä¸€äº›å…³é”®å­—
                if table.upper() not in ['SELECT', 'WHERE', 'JOIN', 'INNER', 'LEFT', 'RIGHT', 'OUTER']:
                    return table
        
        # UPDATEè¯­å¥
        update_match = re.search(r'UPDATE\s+`?([a-zA-Z0-9_]+)`?', sql_upper, re.IGNORECASE)
        if update_match:
            return update_match.group(1)
        
        # INSERTè¯­å¥
        insert_match = re.search(r'INSERT\s+INTO\s+`?([a-zA-Z0-9_]+)`?', sql_upper, re.IGNORECASE)
        if insert_match:
            return insert_match.group(1)
        
        # DELETEè¯­å¥
        delete_match = re.search(r'DELETE\s+FROM\s+`?([a-zA-Z0-9_]+)`?', sql_upper, re.IGNORECASE)
        if delete_match:
            return delete_match.group(1)
        
        return None
    
    def _init_langchain_prompts(self):
        """åˆå§‹åŒ–LangChainæç¤ºè¯æ¨¡æ¿"""
        if not LANGCHAIN_AVAILABLE:
            return
        
        # æ…¢æŸ¥è¯¢æ™ºèƒ½ä¼˜åŒ–æç¤ºè¯æ¨¡æ¿ï¼ˆåªç»™å‡ºæœ€ä¼˜æ–¹æ¡ˆï¼‰
        self.optimization_prompt = PromptTemplate(
            input_variables=["sql", "table_structure", "explain_result", "execute_cnt", "query_time", "ip", "db_name"],
            template="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„MySQLæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ…¢æŸ¥è¯¢SQLå¹¶åªç»™å‡ºæœ€ä¼˜çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚

æ…¢æŸ¥è¯¢ä¿¡æ¯ï¼š
- æ•°æ®åº“ä½ç½®: {ip}
- æ•°æ®åº“å: {db_name}
- æŸ¥è¯¢æ‰§è¡Œæ¬¡æ•°: {execute_cnt}
- å¹³å‡æŸ¥è¯¢æ—¶é—´: {query_time}ç§’

SQLè¯­å¥:
{sql}

è¡¨ç»“æ„ä¿¡æ¯:
{table_structure}

EXPLAINæ‰§è¡Œè®¡åˆ’:
{explain_result}

è¯·åªç»™å‡ºæœ€ä¼˜çš„ä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š
1. æ ¸å¿ƒé—®é¢˜ï¼šä¸€å¥è¯è¯´æ˜ä¸»è¦æ€§èƒ½é—®é¢˜
2. æ™ºèƒ½ä¼˜åŒ–å»ºè®®ï¼šæä¾›æœ€æœ‰æ•ˆçš„ä¼˜åŒ–SQLï¼ˆç´¢å¼•åˆ›å»ºè¯­å¥æˆ–æŸ¥è¯¢é‡å†™ï¼‰
3. é¢„æœŸæ•ˆæœï¼šä¼˜åŒ–åçš„æ€§èƒ½æå‡

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç®€æ´æ˜äº†ï¼Œåªç»™å‡ºæœ€ä¼˜æ–¹æ¡ˆï¼Œä¸è¦æä¾›å¤šä¸ªå¤‡é€‰æ–¹æ¡ˆã€‚
"""
        )
    
    def analyze_with_deepseek(self, data: Dict, timeout: int = 60) -> str:
        """
        ä½¿ç”¨DeepSeek APIåˆ†ææ…¢æŸ¥è¯¢SQL
        
        Args:
            data: åŒ…å«SQLå’Œè¡¨ç»“æ„ä¿¡æ¯çš„å­—å…¸
            timeout: APIè¯·æ±‚è¶…æ—¶æ—¶é—´
            
        Returns:
            åˆ†æç»“æœæ–‡æœ¬
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepSeek APIåˆ†ææ…¢æŸ¥è¯¢")
            
            # éªŒè¯å¿…è¦å‚æ•°
            sql_content = data.get('sql', '')
            if not sql_content:
                logger.warning("SQLå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
                return ""
            
            # æ„å»ºæç¤ºè¯
            if LANGCHAIN_AVAILABLE and hasattr(self, 'optimization_prompt'):
                # ä½¿ç”¨LangChainæ ¼å¼åŒ–æç¤ºè¯
                try:
                    prompt_text = self.optimization_prompt.format(
                        sql=data.get('sql', ''),
                        table_structure=json.dumps(data.get('table_structure', {}), ensure_ascii=False, indent=2),
                        explain_result=json.dumps(data.get('explain_result', {}), ensure_ascii=False, indent=2),
                        execute_cnt=data.get('execute_cnt', 0),
                        query_time=data.get('query_time', 0.0),
                        ip=data.get('ip', ''),
                        db_name=data.get('db_name', '')
                    )
                    logger.debug("ä½¿ç”¨LangChainæ ¼å¼åŒ–æç¤ºè¯æˆåŠŸ")
                except Exception as e:
                    logger.error(f"LangChainæç¤ºè¯æ ¼å¼åŒ–å¤±è´¥: {str(e)}")
                    # é™çº§åˆ°ç›´æ¥æ„å»ºæç¤ºè¯
                    prompt_text = self._build_fallback_prompt(data)
            else:
                # ç›´æ¥æ„å»ºæç¤ºè¯
                prompt_text = self._build_fallback_prompt(data)
            
            url = "https://api.deepseek.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.deepseek_api_key}",
                "Content-Type": "application/json"
            }
            
            # è°ƒç”¨DeepSeek API
            logger.info("å‘é€è¯·æ±‚åˆ°DeepSeek API")
            response = requests.post(
                url,
                headers=headers,
                json={
                    "model": "deepseek-chat",
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„MySQLæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ï¼Œåªç»™å‡ºæœ€ä¼˜çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œä¸è¦æä¾›å¤šä¸ªå¤‡é€‰æ–¹æ¡ˆã€‚"},
                        {"role": "user", "content": prompt_text}
                    ],
                    "temperature": 0.3,
                    "max_tokens": 4000
                },
                timeout=timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                logger.info("DeepSeek APIåˆ†ææˆåŠŸ")
                content = result['choices'][0]['message']['content']
                # ç¡®ä¿å†…å®¹æ­£ç¡®ç¼–ç 
                if isinstance(content, str):
                    return content.encode('utf-8').decode('utf-8')
                return str(content)
            else:
                error_msg = f"APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}, {response.text}"
                logger.error(error_msg)
                return error_msg
                
        except requests.exceptions.Timeout:
            error_msg = f"APIè¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡{timeout}ç§’ï¼‰"
            logger.error(error_msg)
            return error_msg
        except requests.RequestException as e:
            error_msg = f"ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return error_msg
        except json.JSONDecodeError as e:
            error_msg = f"APIå“åº”è§£æå¤±è´¥: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return error_msg
        except Exception as e:
            error_msg = f"DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return error_msg
            
    def _build_fallback_prompt(self, data: Dict) -> str:
        """æ„å»ºå¤‡ç”¨æç¤ºè¯ï¼Œå½“LangChainä¸å¯ç”¨æˆ–æ ¼å¼åŒ–å¤±è´¥æ—¶ä½¿ç”¨"""
        # è·å–è¡¨å¼•æ“ä¿¡æ¯
        table_structure = data.get('table_structure', {})
        table_status = table_structure.get('table_status', {})
        actual_engine = table_status.get('engine', 'æœªçŸ¥')
        
        return f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„MySQLæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ…¢æŸ¥è¯¢SQLå¹¶åªç»™å‡ºæœ€ä¼˜çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚
  
  æ…¢æŸ¥è¯¢ä¿¡æ¯ï¼š
  SQLè¯­å¥: {data.get('sql', '')}
  è¡¨ç»“æ„: {json.dumps(table_structure, ensure_ascii=False, indent=2)}
  æ‰§è¡Œè®¡åˆ’: {json.dumps(data.get('explain_result', {}), ensure_ascii=False, indent=2)}
  æ‰§è¡Œæ¬¡æ•°: {data.get('execute_cnt', 0)}
  æŸ¥è¯¢æ—¶é—´: {data.get('query_time', 0.0)}ms
  æ•°æ®åº“IP: {data.get('ip', '')}
  æ•°æ®åº“åç§°: {data.get('db_name', '')}
  è¡¨å¼•æ“: {actual_engine}
  
  è¯·è¾“å‡ºä»¥ä¸‹ä¸‰éƒ¨åˆ†å†…å®¹ï¼š
  1. æ ¸å¿ƒé—®é¢˜ï¼šç®€è¦è¯´æ˜SQLæ€§èƒ½æ…¢çš„æ ¹æœ¬åŸå› 
  2. ä¼˜åŒ–æ–¹æ¡ˆï¼šåªç»™å‡ºæœ€ä¼˜çš„SQLä¼˜åŒ–æ–¹æ¡ˆ
  3. é¢„æœŸæ•ˆæœï¼šä¼˜åŒ–åé¢„è®¡æå‡çš„æ€§èƒ½
  
  è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç®€æ´æ˜äº†ï¼Œåªç»™å‡ºæœ€ä¼˜æ–¹æ¡ˆï¼Œä¸è¦æä¾›å¤šä¸ªå¤‡é€‰æ–¹æ¡ˆã€‚
  """
    
    def _get_database_config(self, hostname: str, db_name: str) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“é…ç½®"""
        try:
            # ä½¿ç”¨å½“å‰è¿æ¥çš„é…ç½®ä½œä¸ºåŸºç¡€
            config = {
                'host': hostname or self.slow_query_db_host,
                'port': self.slow_query_db_port,
                'user': self.slow_query_db_user,
                'password': self.slow_query_db_password,
                'database': db_name,
                'charset': 'utf8mb4',
                'cursorclass': pymysql.cursors.DictCursor
            }
            return config
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
            return None
    
    def _get_table_structure(self, db_config: Dict, table_name: str) -> Dict[str, Any]:
        """è·å–è¡¨ç»“æ„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç´¢å¼•ä¿¡æ¯"""
        try:
            # åˆ›å»ºæ–°çš„æ•°æ®åº“è¿æ¥
            connection = pymysql.connect(**db_config)
            
            with connection.cursor() as cursor:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                db_name = db_config.get('database', '')
                cursor.execute(f"SHOW TABLES LIKE '{table_name}'")
                if not cursor.fetchone():
                    logger.warning(f"è¡¨ä¸å­˜åœ¨: {db_name}.{table_name}")
                    return {'error': 'table_not_found'}
                
                # è·å–è¡¨çŠ¶æ€ä¿¡æ¯
                cursor.execute(f"SHOW TABLE STATUS FROM `{db_name}` WHERE Name = '{table_name}'")
                table_status = cursor.fetchone() or {}
                
                # è·å–åˆ—ä¿¡æ¯
                cursor.execute(f"SHOW FULL COLUMNS FROM `{db_name}`.`{table_name}`")
                columns_info = {}
                for row in cursor.fetchall():
                    column_name = row['Field']
                    columns_info[column_name] = {
                        'type': row['Type'],
                        'null': row['Null'],
                        'key': row['Key'],
                        'default': row['Default'],
                        'extra': row['Extra'],
                        'primary_key': row['Key'] == 'PRI',
                        'index': row['Key'] in ['MUL', 'UNI', 'PRI']
                    }
                
                # è·å–ç´¢å¼•ä¿¡æ¯
                cursor.execute(f"SHOW INDEX FROM `{db_name}`.`{table_name}`")
                indexes_info = {}
                for row in cursor.fetchall():
                    index_name = row['Key_name']
                    if index_name not in indexes_info:
                        indexes_info[index_name] = {
                            'name': index_name,
                            'unique': not row['Non_unique'],
                            'primary': row['Key_name'] == 'PRIMARY',
                            'columns': []
                        }
                    indexes_info[index_name]['columns'].append(row['Column_name'])
                
                connection.close()
                
                return {
                    'table_status': table_status,
                    'columns': columns_info,
                    'indexes': indexes_info,
                    'database': db_name,
                    'table': table_name
                }
                
        except Exception as e:
            # å¤„ç†æ•°æ®åº“ç›¸å…³é”™è¯¯
            error_code = getattr(e, 'args', [0])[0] if hasattr(e, 'args') else 0
            error_msg = str(e)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯MySQLé”™è¯¯
            if 'MySQL' in str(type(e)) or error_code in [1146, 1049]:
                logger.error(f"è·å–è¡¨ç»“æ„å¤±è´¥ (é”™è¯¯ç : {error_code}): {error_msg}")
                
                if error_code == 1146:  # Table doesn't exist
                    return {'error': 'table_not_found'}
                elif error_code == 1049:  # Unknown database
                    return {'error': 'database_not_found'}
                else:
                    return {'error': error_msg}
            else:
                # éMySQLé”™è¯¯ï¼Œè®°å½•åè¿”å›
                logger.error(f"è·å–è¡¨ç»“æ„æ—¶å‡ºé”™: {error_msg}")
                return {'error': error_msg}
        except Exception as e:
            logger.error(f"è·å–è¡¨ç»“æ„æ—¶å‡ºé”™: {e}")
            return {'error': str(e)}
    
    def _get_explain_result(self, db_config: Dict, sql_content: str) -> Dict[str, Any]:
        """è·å–EXPLAINç»“æœ"""
        # ç¡®ä¿ä½¿ç”¨å…¨å±€å¯¼å…¥çš„pymysql
        global pymysql
        connection = None
        
        try:
            # åˆ›å»ºæ–°çš„æ•°æ®åº“è¿æ¥
            connection = pymysql.connect(**db_config)
            
            with connection.cursor() as cursor:
                # æ‰§è¡ŒEXPLAIN
                cursor.execute(f"EXPLAIN {sql_content}")
                explain_rows = cursor.fetchall()
                
                # åˆ†æEXPLAINç»“æœ
                analysis = {
                    'rows_examined': 0,
                    'using_filesort': False,
                    'using_temporary': False,
                    'possible_keys': [],
                    'used_key': None,
                    'type': None,
                    'rows': []
                }
                
                for row in explain_rows:
                    analysis['rows'].append(row)
                    
                    # ç»Ÿè®¡æ‰«æè¡Œæ•°
                    if 'rows' in row:
                        analysis['rows_examined'] += int(row['rows'])
                    
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–‡ä»¶æ’åº
                    if 'Extra' in row and row['Extra']:
                        if 'Using filesort' in row['Extra']:
                            analysis['using_filesort'] = True
                        if 'Using temporary' in row['Extra']:
                            analysis['using_temporary'] = True
                    
                    # è·å–å¯èƒ½çš„ç´¢å¼•å’Œä½¿ç”¨çš„ç´¢å¼•
                    if 'possible_keys' in row and row['possible_keys']:
                        analysis['possible_keys'].extend(row['possible_keys'].split(','))
                    
                    if 'key' in row and row['key']:
                        analysis['used_key'] = row['key']
                    
                    if 'type' in row and row['type']:
                        analysis['type'] = row['type']
                
                return analysis
                
        except Exception as e:
            # å¤„ç†æ•°æ®åº“ç›¸å…³é”™è¯¯
            error_code = getattr(e, 'args', [0])[0] if hasattr(e, 'args') else 0
            error_msg = str(e)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯MySQLé”™è¯¯
            if 'MySQL' in str(type(e)):
                logger.error(f"EXPLAINæ‰§è¡Œå¤±è´¥ (é”™è¯¯ç : {error_code}): {error_msg}")
                return {'error': error_msg}
            else:
                # éMySQLé”™è¯¯ï¼Œè®°å½•åè¿”å›
                logger.error(f"è·å–EXPLAINç»“æœæ—¶å‡ºé”™: {error_msg}")
                return {'error': error_msg}
        finally:
            # ç¡®ä¿è¿æ¥å…³é—­
            if connection and hasattr(connection, 'close'):
                try:
                    connection.close()
                except Exception:
                    pass
    
    def _get_deepseek_optimization_suggestions(self, sql_content: str, table_structure: Dict, explain_result: Dict) -> List[str]:
        """è·å–DeepSeek APIçš„ä¼˜åŒ–å»ºè®®"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if table_structure.get('error') == 'table_not_found':
                return ["âŒ è¡¨ä¸å­˜åœ¨: è¯·ç¡®è®¤æ•°æ®åº“åå’Œè¡¨åæ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…è¯¥è¡¨å¯èƒ½å·²è¢«åˆ é™¤"]
            
            # åˆ†æç°æœ‰ç´¢å¼•
            existing_indexes = []
            if table_structure.get('indexes'):
                for index_name, index_info in table_structure['indexes'].items():
                    if not index_info.get('primary', False):  # æ’é™¤ä¸»é”®
                        existing_indexes.append(f"{index_name}({', '.join(index_info['columns'])})")
            
            # åˆ†æEXPLAINç»“æœ
            explain_analysis = []
            if explain_result.get('rows_examined', 0) > 1000:
                explain_analysis.append(f"æ‰«æè¡Œæ•°è¿‡å¤š({explain_result['rows_examined']}è¡Œ)")
            
            if explain_result.get('using_filesort'):
                explain_analysis.append("ä½¿ç”¨äº†æ–‡ä»¶æ’åº")
            
            if explain_result.get('using_temporary'):
                explain_analysis.append("ä½¿ç”¨äº†ä¸´æ—¶è¡¨")
            
            if not explain_result.get('used_key'):
                explain_analysis.append("æœªä½¿ç”¨ç´¢å¼•")
            
            # ç”Ÿæˆå»ºè®®
            suggestions = []
            
            # ç´¢å¼•åˆ†æ
            if existing_indexes:
                suggestions.append(f"âœ… å·²å­˜åœ¨ç´¢å¼•: {', '.join(existing_indexes)}")
                
                # å¦‚æœå·²æœ‰ç´¢å¼•ä½†æŸ¥è¯¢ä»ç„¶æ…¢ï¼Œæä¾›å…¶ä»–å»ºè®®
                if explain_analysis:
                    suggestions.append("ğŸ’¡ è™½ç„¶å­˜åœ¨ç´¢å¼•ï¼Œä½†æŸ¥è¯¢ä»æœ‰ä¼˜åŒ–ç©ºé—´:")
                    for issue in explain_analysis:
                        suggestions.append(f"   - {issue}")
                    
                    # æä¾›å…·ä½“ä¼˜åŒ–å»ºè®®
                    if "æ‰«æè¡Œæ•°è¿‡å¤š" in str(explain_analysis):
                        suggestions.append("ğŸ’¡ å»ºè®®: ä¼˜åŒ–WHEREæ¡ä»¶ï¼Œå‡å°‘æ‰«æèŒƒå›´")
                    
                    if "ä½¿ç”¨äº†æ–‡ä»¶æ’åº" in str(explain_analysis):
                        suggestions.append("ğŸ’¡ å»ºè®®: è€ƒè™‘æ·»åŠ ORDER BYå­—æ®µçš„å¤åˆç´¢å¼•")
                    
                    if "æœªä½¿ç”¨ç´¢å¼•" in str(explain_analysis):
                        suggestions.append("ğŸ’¡ å»ºè®®: åˆ†æWHEREæ¡ä»¶ï¼Œç¡®ä¿ç´¢å¼•è¢«æœ‰æ•ˆä½¿ç”¨")
                else:
                    suggestions.append("ğŸ¯ å½“å‰æŸ¥è¯¢å·²ç»æ˜¯æœ€ä¼˜çŠ¶æ€ï¼Œæ— éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
            else:
                suggestions.append("ğŸ” æœªæ‰¾åˆ°åˆé€‚çš„ç´¢å¼•ï¼Œå»ºè®®åˆ†ææŸ¥è¯¢æ¨¡å¼æ·»åŠ ç´¢å¼•")
            
            return suggestions
            
        except Exception as e:
            logger.error(f"ç”ŸæˆDeepSeekä¼˜åŒ–å»ºè®®å¤±è´¥: {e}")
            return [f"ç”Ÿæˆä¼˜åŒ–å»ºè®®æ—¶å‡ºé”™: {str(e)}"]
    
    def _analyze_slow_query(self, sql_data: Dict) -> Dict:
        """
        åˆ†æå•æ¡æ…¢æŸ¥è¯¢SQL
        
        Args:
            sql_data: åŒ…å«SQLä¿¡æ¯çš„å­—å…¸
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            sql_content = sql_data.get('sql_content', '')
            table_name = sql_data.get('table_name', '')
            db_name = sql_data.get('db_name', '')
            
            if not sql_content or not table_name:
                logger.warning("SQLå†…å®¹æˆ–è¡¨åä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
                return {}
            
            logger.info(f"å¼€å§‹åˆ†ææ…¢æŸ¥è¯¢: {sql_content[:50]}... (è¡¨: {table_name})")
            
            # æå–æ•°æ®åº“å
            extracted_db, _ = extract_db_table_from_sql(sql_content)
            final_db_name = extracted_db or db_name or 'db'
            
            # è·å–æ•°æ®åº“é…ç½®
            db_config = self._get_database_config(sql_data.get('hostname_max', ''), final_db_name)
            
            # è·å–è¡¨ç»“æ„ä¿¡æ¯
            table_structure = self._get_table_structure(db_config, table_name)
            
            # è·å–EXPLAINç»“æœ
            explain_result = self._get_explain_result(db_config, sql_content)
            
            # è·å–DeepSeekä¼˜åŒ–å»ºè®®
            optimization_suggestions = self._get_deepseek_optimization_suggestions({
                'sql': sql_content,
                'table_structure': table_structure,
                'explain_result': explain_result,
                'execute_cnt': sql_data.get('execute_cnt', 0),
                'query_time': sql_data.get('query_time', 0.0),
                'ip': sql_data.get('ip', ''),
                'db_name': final_db_name
            })
            
            return {
                'sql': sql_content,
                'table_name': table_name,
                'db_name': final_db_name,
                'table_structure': table_structure,
                'explain_result': explain_result,
                'optimization_suggestions': optimization_suggestions,
                'analysis_status': 'success'
            }
            
        except Exception as e:
            logger.error(f"åˆ†ææ…¢æŸ¥è¯¢å¤±è´¥: {str(e)}", exc_info=True)
            return {
                'sql': sql_content,
                'table_name': table_name,
                'db_name': db_name,
                'error': str(e),
                'analysis_status': 'failed'
            }
    
    def _analyze_slow_query(self, slow_query_info: Dict) -> Dict:
        """
        åˆ†æå•æ¡æ…¢æŸ¥è¯¢ï¼Œæå–è¡¨åï¼Œè·å–æ•°æ®åº“é…ç½®ï¼Œè°ƒç”¨ä¼˜åŒ–å™¨
        
        Args:
            slow_query_info: æ…¢æŸ¥è¯¢ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ipã€db_nameã€sql_contentã€execute_cntã€query_timeç­‰å­—æ®µ
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            # æå–æ…¢æŸ¥è¯¢ä¿¡æ¯
            ip = slow_query_info.get('ip', '')
            db_name = slow_query_info.get('db_name', '')
            sql_content = slow_query_info.get('sql_content', '')
            execute_cnt = slow_query_info.get('execute_cnt', 0)
            query_time = slow_query_info.get('query_time', 0.0)
            
            # å¦‚æœdb_nameä¸ºç©ºï¼Œå°è¯•ä»SQLä¸­æå–
            if not db_name:
                extracted_db, extracted_table = extract_db_table_from_sql(sql_content)
                db_name = extracted_db or 'unknown_db'
            
            # è·å–è¡¨å
            table_name = self.extract_table_name(sql_content)
            if not table_name:
                table_name = 'unknown_table'
            
            # è·å–æ•°æ®åº“é…ç½®
            db_config = self._get_database_config(ip, db_name)
            if not db_config:
                logger.warning(f"æœªæ‰¾åˆ°æ•°æ®åº“é…ç½®: {ip}:{db_name}")
                db_config = {
                    'host': ip,
                    'port': 3306,
                    'user': '',
                    'password': '',
                    'database': db_name
                }
            
            # è·å–è¡¨ç»“æ„å’Œç´¢å¼•ä¿¡æ¯
            table_structure = self._get_table_structure(db_config, table_name)
            
            # è·å–EXPLAINç»“æœ
            explain_result = self._get_explain_result(db_config, sql_content)
            
            # è°ƒç”¨DeepSeek APIè·å–ä¼˜åŒ–å»ºè®®
            deepseek_optimization = self._get_deepseek_optimization_suggestions(
                sql_content, table_structure, explain_result
            )
            
            # æ„å»ºåˆ†æç»“æœ
            result = {
                'sql': sql_content,
                'database': db_name,
                'table': table_name,
                'table_structure': table_structure,
                'explain_result': explain_result,
                'deepseek_optimization': deepseek_optimization,
                'slow_query_info': slow_query_info,
                'analysis_time': datetime.now().isoformat()
            }
            
            # ä½¿ç”¨å¢å¼ºç‰ˆæŠ¥å‘Šè¾“å‡º
            self._print_enhanced_report(result)
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ…¢æŸ¥è¯¢è®°å½•å¤±è´¥: {str(e)}")
            return None

    def compare_slow_queries(self, min_execute_cnt: int = 10, min_query_time: float = 10.0) -> Dict:
        """
        å¯¹æ¯”åˆ†æä¸Šä¸ªæœˆå’Œä¸Šä¸Šä¸ªæœˆæ…¢æŸ¥è¯¢æ•°æ®
        
        Args:
            min_execute_cnt: æœ€å°æŸ¥è¯¢æ¬¡æ•°
            min_query_time: æœ€å°æŸ¥è¯¢æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            åŒ…å«ä¸¤ä¸ªæœˆå¯¹æ¯”æ•°æ®çš„å­—å…¸
        """
        try:
            # è·å–ä¸Šä¸ªæœˆçš„æ•°æ®
            last_month_queries = self.get_slow_queries(min_execute_cnt, min_query_time, month_offset=1)
            
            # è·å–ä¸Šä¸Šä¸ªæœˆçš„æ•°æ®
            previous_month_queries = self.get_slow_queries(min_execute_cnt, min_query_time, month_offset=2)
            
            
            # å¦‚æœä¸Šä¸Šä¸ªæœˆæ²¡æœ‰æ•°æ®ï¼Œä¿æŒç©ºåˆ—è¡¨ï¼Œä¸ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            # ç¡®ä¿æ•°æ®å‡†ç¡®æ€§ï¼šæ²¡æœ‰æ•°æ®å°±æ˜¾ç¤º0
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            last_month_total = len(last_month_queries)
            previous_month_total = len(previous_month_queries)
            
            # è®¡ç®—å¢é•¿ç‡
            growth_rate = 0
            if previous_month_total > 0:
                growth_rate = ((last_month_total - previous_month_total) / previous_month_total) * 100
            elif last_month_total > 0:
                growth_rate = 100
            
            # è®¡ç®—æ•°é‡å˜åŒ–ï¼ˆç”¨äºæŠ¥å‘Šç”Ÿæˆå™¨å…¼å®¹ï¼‰
            count_change = growth_rate
            
            # æ‰¾å‡ºæ–°å¢çš„æ…¢æŸ¥è¯¢ï¼ˆé€šè¿‡SQLå†…å®¹æ¯”è¾ƒï¼‰
            last_month_sqls = set(query['sql_content'] for query in last_month_queries)
            previous_month_sqls = set(query['sql_content'] for query in previous_month_queries)
            
            new_slow_queries = last_month_sqls - previous_month_sqls
            resolved_slow_queries = previous_month_sqls - last_month_sqls
            
            # è®¡ç®—å¹³å‡æŸ¥è¯¢æ—¶é—´
            last_month_avg_time = sum(query['query_time'] for query in last_month_queries) / max(1, last_month_total)
            previous_month_avg_time = sum(query['query_time'] for query in previous_month_queries) / max(1, previous_month_total)
            
            # è®¡ç®—å¹³å‡æ‰§è¡Œæ¬¡æ•°
            last_month_avg_count = sum(query['execute_cnt'] for query in last_month_queries) / max(1, last_month_total)
            previous_month_avg_count = sum(query['execute_cnt'] for query in previous_month_queries) / max(1, previous_month_total)
            
            # è·å–æœ€è€—æ—¶çš„æ…¢æŸ¥è¯¢TOP5
            last_month_top5 = sorted(last_month_queries, key=lambda x: x['query_time'], reverse=True)[:5]
            previous_month_top5 = sorted(previous_month_queries, key=lambda x: x['query_time'], reverse=True)[:5]
            
            # è·å–æ‰§è¡Œæ¬¡æ•°æœ€å¤šçš„æ…¢æŸ¥è¯¢TOP5
            last_month_most_freq = sorted(last_month_queries, key=lambda x: x['execute_cnt'], reverse=True)[:5]
            previous_month_most_freq = sorted(previous_month_queries, key=lambda x: x['execute_cnt'], reverse=True)[:5]
            
            # è·å–æ—¥æœŸä¿¡æ¯
            today = datetime.now()
            first_day_of_current_month = today.replace(day=1)
            last_day_of_previous_month = first_day_of_current_month - timedelta(days=1)
            first_day_of_previous_month = last_day_of_previous_month.replace(day=1)
            
            # ä½¿ç”¨æ•°å­—æ ¼å¼é¿å…ä¸­æ–‡ç¼–ç é—®é¢˜ï¼Œç„¶åæ‰‹åŠ¨æ·»åŠ ä¸­æ–‡
            last_month_year = last_day_of_previous_month.strftime('%Y')
            last_month_month = last_day_of_previous_month.strftime('%m')
            last_month_name = f"{last_month_year}å¹´{last_month_month}æœˆ"
            
            # è®¡ç®—ä¸Šä¸Šä¸ªæœˆ
            first_day_of_last_month = first_day_of_previous_month
            last_day_of_two_months_ago = first_day_of_last_month - timedelta(days=1)
            previous_month_year = last_day_of_two_months_ago.strftime('%Y')
            previous_month_month = last_day_of_two_months_ago.strftime('%m')
            previous_month_name = f"{previous_month_year}å¹´{previous_month_month}æœˆ"
            
            return {
                'last_month': {
                    'name': last_month_name,
                    'total': last_month_total,
                    'total_count': last_month_total,  # å…¼å®¹æŠ¥å‘Šç”Ÿæˆå™¨
                    'avg_query_time': last_month_avg_time,
                    'avg_execute_cnt': last_month_avg_count,
                    'top5_by_time': last_month_top5,
                    'top5_by_count': last_month_most_freq,
                    'queries': last_month_queries
                },
                'previous_month': {
                    'name': previous_month_name,
                    'total': previous_month_total,
                    'total_count': previous_month_total,  # å…¼å®¹æŠ¥å‘Šç”Ÿæˆå™¨
                    'avg_query_time': previous_month_avg_time,
                    'avg_execute_cnt': previous_month_avg_count,
                    'top5_by_time': previous_month_top5,
                    'top5_by_count': previous_month_most_freq,
                    'queries': previous_month_queries
                },
                'comparison': {
                    'growth_rate': growth_rate,
                    'count_change': count_change,  # å…¼å®¹æŠ¥å‘Šç”Ÿæˆå™¨
                    'new_queries_count': len(new_slow_queries),
                    'resolved_queries_count': len(resolved_slow_queries)
                }
            }
        except Exception as e:
            logger.error(f"å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            print(f"âœ— å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
            return None
    
    def analyze_all_slow_queries(self, min_execute_cnt: int = 10, min_query_time: float = 10.0) -> Dict:
        """
        è·å–å¹¶åˆ†ææ‰€æœ‰æ…¢æŸ¥è¯¢ï¼Œç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        
        Args:
            min_execute_cnt: æœ€å°æ‰§è¡Œæ¬¡æ•°
            min_query_time: æœ€å°æŸ¥è¯¢æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            åˆ†æç»“æœæ±‡æ€»
        """
        print(f"\nå¼€å§‹åˆ†ææ…¢æŸ¥è¯¢...")
        print(f"ç­›é€‰æ¡ä»¶ï¼šæ‰§è¡Œæ¬¡æ•° > {min_execute_cnt} ä¸” æŸ¥è¯¢æ—¶é—´ > {min_query_time}ç§’")
        
        # è·å–æ…¢æŸ¥è¯¢è®°å½•
        slow_queries = self.get_slow_queries(min_execute_cnt, min_query_time)
        
        if not slow_queries:
            print("âš  æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ…¢æŸ¥è¯¢SQL")
            print("å¯èƒ½çš„åŸå› ï¼š")
            print("  1. è¡¨ä¸­æ²¡æœ‰æ•°æ®")
            print("  2. æ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„è®°å½•ï¼ˆæŸ¥è¯¢æ¬¡æ•°>{} ä¸” æŸ¥è¯¢æ—¶é—´>{}ç§’ï¼‰".format(min_execute_cnt, min_query_time))
            print("  3. è¡¨ç»“æ„ä¸æ­£ç¡®ï¼ˆéœ€è¦åŒ…å«ï¼šip, db_name, sql_content, execute_cnt, query_timeï¼‰")
            return {}
        
        print(f"æ‰¾åˆ° {len(slow_queries)} æ¡æ…¢æŸ¥è¯¢è®°å½•")
        
        # åˆ†ææ‰€æœ‰æ…¢æŸ¥è¯¢
        results = []
        success_count = 0
        
        for i, slow_query in enumerate(slow_queries, 1):
            print(f"\n[{i}/{len(slow_queries)}] åˆ†ææ…¢æŸ¥è¯¢...")
            try:
                result = self._analyze_slow_query(slow_query)
                if result and 'error' not in result:
                    results.append(result)
                    success_count += 1
                else:
                    logger.warning(f"åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            except Exception as e:
                logger.error(f"å¤„ç†æ…¢æŸ¥è¯¢è®°å½•å¤±è´¥: {str(e)}")
                continue
        
        print(f"\næˆåŠŸå¤„ç† {success_count} æ¡æ…¢æŸ¥è¯¢è®°å½•")
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = {
            'total_queries': len(slow_queries),
            'successful_analyses': success_count,
            'failed_analyses': len(slow_queries) - success_count,
            'results': results,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = f"slow_query_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, cls=DateTimeEncoder)
        
        print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return summary

    def _print_enhanced_report(self, result: Dict):
        """
        æ‰“å°å¢å¼ºç‰ˆä¼˜åŒ–æŠ¥å‘Šï¼ˆç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹ï¼‰
        
        Args:
            result: åˆ†æç»“æœå­—å…¸
        """
        slow_info = result.get('slow_query_info', {})
        db_name = result.get('database', slow_info.get('db_name', 'N/A'))
        sql_content = result.get('sql', 'N/A')
        execute_cnt = slow_info.get('execute_cnt', 0)
        query_time = slow_info.get('query_time', 0.0)
        
        # ç®€æ´ä¿¡æ¯å±•ç¤º
        print(f"åº“: {db_name} | æ‰§è¡Œæ—¶é—´: {query_time}ç§’ | æ‰§è¡Œæ¬¡æ•°: {execute_cnt}")
        print(f"SQL: {sql_content}")

        # ç”Ÿæˆæ™ºèƒ½ä¼˜åŒ–å»ºè®®
        intelligent_suggestions = self._generate_intelligent_optimization_suggestions(result)
        
        if intelligent_suggestions:
            print("æ™ºèƒ½åˆ†æå»ºè®®ï¼š")
            for suggestion in intelligent_suggestions:
                print(f"  {suggestion}")
        else:
            deepseek_analysis = result.get('deepseek_optimization') or result.get('optimization_suggestions', '')
            if deepseek_analysis:
                self._print_concise_optimization(deepseek_analysis)
            else:
                print("å»ºè®®: æš‚æ— ")

    def _generate_intelligent_optimization_suggestions(self, result: Dict) -> List[str]:
        """
        ç”Ÿæˆæ™ºèƒ½ä¼˜åŒ–å»ºè®®ï¼ŒåŒ…å«ç´¢å¼•åˆ†æã€SQLæ¨¡å¼è¯†åˆ«ç­‰
        
        Args:
            result: åˆ†æç»“æœå­—å…¸
            
        Returns:
            ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        suggestions = []
        
        sql_content = result.get('sql', '')
        table_structure = result.get('table_structure', {})
        explain_result = result.get('explain_result', {})
        slow_info = result.get('slow_query_info', {})
        table_name = result.get('table', '') or self._extract_table_name_from_sql(sql_content)
        
        if not sql_content:
            return suggestions
        
        # æå–è¡¨åï¼Œå¦‚æœæœªçŸ¥ä½¿ç”¨å ä½ç¬¦
        if not table_name:
            table_name = 'your_table_name'
        
        # 1. ç´¢å¼•åˆ†æ - ç”Ÿæˆå…·ä½“å¯æ‰§è¡Œçš„ç´¢å¼•åˆ›å»ºè¯­å¥
        missing_indexes = self._analyze_missing_indexes(sql_content, table_structure, explain_result)
        if missing_indexes.get('missing_indexes'):
            suggestions.append("\nğŸ”¥ã€å…·ä½“å¯æ‰§è¡Œçš„ç´¢å¼•åˆ›å»ºSQLè¯­å¥ã€‘ï¼š")
            suggestions.append("```sql")
            for i, field in enumerate(missing_indexes['missing_indexes'], 1):
                # æ™ºèƒ½åˆ¤æ–­å­—æ®µç±»å‹å¹¶ç”Ÿæˆç›¸åº”çš„ç´¢å¼•è¯­å¥
                field_lower = field.lower()
                
                # ä¸»é”®å­—æ®µ
                if field_lower in ['id', 'pk', 'primary_key'] or field_lower.endswith('_id'):
                    suggestions.append(f"-- {i}. æ ¸å¿ƒä¸»é”®ç´¢å¼•ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰")
                    suggestions.append(f"ALTER TABLE {table_name} ADD PRIMARY KEY ({field});")
                # æ—¶é—´å­—æ®µ
                elif 'time' in field_lower or 'date' in field_lower or field_lower in ['created', 'updated', 'modified']:
                    suggestions.append(f"-- {i}. æ—¶é—´èŒƒå›´ç´¢å¼•ï¼ˆä¼˜åŒ–æ—¶é—´ç­›é€‰ï¼‰")
                    suggestions.append(f"CREATE INDEX idx_{table_name}_{field}_time ON {table_name}({field});")
                # çŠ¶æ€å­—æ®µ
                elif field_lower in ['status', 'state', 'type', 'flag'] or field_lower.endswith('_status'):
                    suggestions.append(f"-- {i}. çŠ¶æ€ç±»å‹ç´¢å¼•ï¼ˆä¼˜åŒ–åˆ†ç±»æŸ¥è¯¢ï¼‰")
                    suggestions.append(f"CREATE INDEX idx_{table_name}_{field}_status ON {table_name}({field});")
                # æ™®é€šå­—æ®µ
                else:
                    suggestions.append(f"-- {i}. å•åˆ—ç´¢å¼•ï¼ˆåŸºç¡€ä¼˜åŒ–ï¼‰")
                    suggestions.append(f"CREATE INDEX idx_{table_name}_{field} ON {table_name}({field});")
                suggestions.append("")  # ç©ºè¡Œåˆ†éš”
            
            # å¦‚æœæœ‰å¤šä¸ªå­—æ®µï¼Œå»ºè®®å¤åˆç´¢å¼•
            if len(missing_indexes['missing_indexes']) > 1:
                fields = missing_indexes['missing_indexes'][:3]  # æœ€å¤š3ä¸ªå­—æ®µ
                fields_str = ', '.join(fields)
                suggestions.append(f"-- ğŸ”¥ å¤åˆç´¢å¼•ï¼ˆå¤šæ¡ä»¶æŸ¥è¯¢æ ¸å¿ƒä¼˜åŒ–ï¼‰")
                suggestions.append(f"CREATE INDEX idx_{'_'.join(fields)}_composite ON {table_name}({fields_str});")
                suggestions.append("")
            
            # æ·»åŠ ç´¢å¼•éªŒè¯å’Œåˆ†æè¯­å¥
            suggestions.append("-- âœ… ç´¢å¼•åˆ›å»ºåçš„éªŒè¯æ­¥éª¤")
            suggestions.append(f"SHOW INDEX FROM {table_name};")
            suggestions.append(f"EXPLAIN FORMAT=JSON {sql_content};")
            suggestions.append(f"ANALYZE TABLE {table_name};")
            suggestions.append("```")
        
        # 2. æ£€æŸ¥å·²å­˜åœ¨ç´¢å¼•ï¼ˆé¿å…é‡å¤å»ºè®®ï¼‰
        existing_indexes = self._check_existing_indexes(sql_content, table_structure)
        if existing_indexes.get('existing_indexes'):
            for field in existing_indexes['existing_indexes']:
                suggestions.append(f"ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ å­—æ®µ `{field}` å·²æœ‰ç´¢å¼•è¦†ç›–ï¼Œæ— éœ€é‡å¤åˆ›å»ºï¼Œç´¢å¼•ä¼˜åŒ–å·²åˆ°ä½")
        
        # 3. SQLæ¨¡å¼åˆ†æ - æä¾›å…·ä½“çš„ä¼˜åŒ–SQLç¤ºä¾‹
        sql_patterns = self._analyze_sql_patterns(sql_content)
        pattern_optimizations = []
        
        if sql_patterns.get('select_all'):
            pattern_optimizations.append("âš ï¸  é¿å…ä½¿ç”¨SELECT *ï¼ŒåªæŸ¥è¯¢éœ€è¦çš„å­—æ®µ")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: SELECT id, name, status FROM {table_name} WHERE ...")
        
        if sql_patterns.get('no_where'):
            pattern_optimizations.append("âš ï¸  æŸ¥è¯¢ç¼ºå°‘WHEREæ¡ä»¶ï¼Œå¯èƒ½å¯¼è‡´å…¨è¡¨æ‰«æ")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: æ·»åŠ WHEREæ¡ä»¶é™åˆ¶æ•°æ®èŒƒå›´")
        
        if sql_patterns.get('complex_join'):
            pattern_optimizations.append("ğŸ”€ å¤šè¡¨JOINæŸ¥è¯¢ï¼Œç¡®ä¿å…³è”å­—æ®µæœ‰ç´¢å¼•")
            pattern_optimizations.append(f"   å…·ä½“ä¼˜åŒ–: CREATE INDEX idx_join_field ON {table_name}(join_field);")
        
        if sql_patterns.get('order_by_rand'):
            pattern_optimizations.append("âš ï¸  é¿å…ä½¿ç”¨ORDER BY RAND()ï¼Œæ€§èƒ½å¼€é”€å¤§")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: ä½¿ç”¨åº”ç”¨å±‚éšæœºæˆ–é¢„å…ˆç”Ÿæˆéšæœºåºå·å­—æ®µ")
        
        if sql_patterns.get('large_offset'):
            pattern_optimizations.append("âš ï¸  å¤§åç§»é‡LIMITï¼Œè€ƒè™‘ä½¿ç”¨æ¸¸æ ‡æˆ–åˆ†é¡µä¼˜åŒ–")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: WHERE id > last_id ORDER BY id LIMIT 100")
        
        if sql_patterns.get('union_distinct'):
            pattern_optimizations.append("âš ï¸  UNIONå»é‡æ“ä½œå¼€é”€å¤§ï¼Œè€ƒè™‘ä½¿ç”¨UNION ALL")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: å°†UNIONæ”¹ä¸ºUNION ALLï¼ˆå¦‚æœç¡®è®¤æ— é‡å¤ï¼‰")
        
        if sql_patterns.get('subquery'):
            pattern_optimizations.append("ğŸ” å­æŸ¥è¯¢å¯èƒ½å½±å“æ€§èƒ½ï¼Œè€ƒè™‘ä½¿ç”¨JOINæ›¿ä»£")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: å°†å­æŸ¥è¯¢æ”¹å†™ä¸ºJOINè¯­å¥")
        
        if sql_patterns.get('group_by_having'):
            pattern_optimizations.append("ğŸ“Š GROUP BY + HAVINGï¼Œç¡®ä¿åˆ†ç»„å­—æ®µæœ‰ç´¢å¼•")
            pattern_optimizations.append(f"   å…·ä½“ä¼˜åŒ–: CREATE INDEX idx_{table_name}_group_field ON {table_name}(group_field);")
        
        if sql_patterns.get('in_clause'):
            pattern_optimizations.append("ğŸ” INå­å¥åŒ…å«å¤šä¸ªå€¼ï¼Œè€ƒè™‘ä½¿ç”¨JOINæˆ–ä¸´æ—¶è¡¨")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: ä½¿ç”¨ä¸´æ—¶è¡¨æˆ–JOINä¼˜åŒ–INæŸ¥è¯¢")
        
        if sql_patterns.get('not_in_clause'):
            pattern_optimizations.append("âš ï¸  NOT INå¯èƒ½å¯¼è‡´å…¨è¡¨æ‰«æï¼Œè€ƒè™‘ä½¿ç”¨LEFT JOINæ›¿ä»£")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: LEFT JOIN ... WHERE right_table.id IS NULL")
        
        if sql_patterns.get('distinct'):
            pattern_optimizations.append("âœ¨ DISTINCTå»é‡æ“ä½œï¼Œè€ƒè™‘æ•°æ®æ¨¡å‹ä¼˜åŒ–")
            pattern_optimizations.append("   ä¼˜åŒ–ç¤ºä¾‹: é€šè¿‡GROUP BYæˆ–å”¯ä¸€ç´¢å¼•é¿å…é‡å¤æ•°æ®")
        
        # å¦‚æœæœ‰SQLæ¨¡å¼ä¼˜åŒ–å»ºè®®ï¼Œæ·»åŠ åˆ°suggestionsä¸­
        if pattern_optimizations:
            suggestions.append("\nğŸ“‹ã€SQLæ¨¡å¼ä¼˜åŒ–å»ºè®®ã€‘ï¼š")
            suggestions.extend(pattern_optimizations)
        
        # 4. æŸ¥è¯¢æ€§èƒ½æ¨¡å¼åˆ†æ
        performance_patterns = self._analyze_performance_patterns(sql_content, slow_info)
        if performance_patterns.get('high_frequency'):
            suggestions.append("ğŸš€ é«˜é¢‘æ…¢æŸ¥è¯¢ï¼Œå»ºè®®ä¼˜å…ˆä¼˜åŒ–å¹¶è€ƒè™‘ç¼“å­˜")
        if performance_patterns.get('long_query'):
            suggestions.append("â° æŸ¥è¯¢æ—¶é—´è¶…è¿‡30ç§’ï¼Œéœ€è¦ç«‹å³ä¼˜åŒ–")
        if performance_patterns.get('complex_or'):
            suggestions.append("ğŸ”€ å¤æ‚ORæ¡ä»¶ï¼Œè€ƒè™‘æ‹†åˆ†ä¸ºå¤šä¸ªæŸ¥è¯¢æˆ–ä½¿ç”¨UNION")
        
        # 5. è¡¨ç»“æ„åˆ†æ
        table_analysis = self._analyze_table_structure(table_structure)
        if table_analysis.get('no_primary_key'):
            suggestions.append("ğŸ”‘ è¡¨ç¼ºå°‘ä¸»é”®ï¼Œå»ºè®®æ·»åŠ è‡ªå¢ä¸»é”®")
            suggestions.append(f"   å…·ä½“ä¼˜åŒ–: ALTER TABLE {table_name} ADD PRIMARY KEY (id);")
        if table_analysis.get('large_text_fields'):
            suggestions.append("ğŸ“ å­˜åœ¨å¤§æ–‡æœ¬å­—æ®µï¼Œè€ƒè™‘å‚ç›´åˆ†è¡¨")
            suggestions.append("   ä¼˜åŒ–ç¤ºä¾‹: å°†å¤§æ–‡æœ¬å­—æ®µæ‹†åˆ†åˆ°ç‹¬ç«‹è¡¨ä¸­")
        if table_analysis.get('no_index'):
            suggestions.append("ğŸ“Š è¡¨ç¼ºå°‘ç´¢å¼•ï¼Œå»ºè®®åˆ†ææŸ¥è¯¢æ¨¡å¼æ·»åŠ ç´¢å¼•")
            suggestions.append(f"   å…·ä½“ä¼˜åŒ–: å‚è€ƒä¸Šé¢çš„ç´¢å¼•åˆ›å»ºSQLè¯­å¥")
        
        # 6. æ·»åŠ æ™ºèƒ½åŒ–çš„æ€§èƒ½æå‡é¢„æœŸ
        if missing_indexes.get('missing_indexes') or pattern_optimizations:
            suggestions.append("\nğŸ”¥ã€AIæ™ºèƒ½æ€§èƒ½é¢„æœŸã€‘ï¼š")
            
            # æ™ºèƒ½è®¡ç®—æ€§èƒ½æå‡é¢„æœŸ
            base_improvement = 60
            where_fields = self._extract_where_fields(sql_content)
            join_fields = self._extract_join_fields(sql_content)
            
            # æ ¹æ®å­—æ®µæ•°é‡è°ƒæ•´
            if len(where_fields) >= 3:
                base_improvement += 25
            elif len(where_fields) == 1:
                base_improvement -= 10
            
            # æ ¹æ®æ˜¯å¦æœ‰JOINè°ƒæ•´
            if join_fields:
                base_improvement += 15
            
            # æ ¹æ®æŸ¥è¯¢é¢‘ç‡è°ƒæ•´
            execute_cnt = slow_info.get('execute_cnt', 0)
            if execute_cnt > 1000:
                base_improvement += 10
            
            # ç¡®ä¿æå‡èŒƒå›´åˆç†
            min_improvement = max(50, base_improvement - 15)
            max_improvement = min(95, base_improvement + 20)
            
            suggestions.append(f"   ğŸ“ˆ æŸ¥è¯¢æ€§èƒ½é¢„è®¡æå‡: {min_improvement}-{max_improvement}%")
            
            # æ™ºèƒ½é¢„æµ‹å“åº”æ—¶é—´æ”¹å–„
            if max_improvement >= 80:
                suggestions.append("   â±ï¸ å“åº”æ—¶é—´: å¹³å‡è€—æ—¶ä»2000msé™ä½è‡³200msï¼Œé¢„è®¡æå‡10å€æ€§èƒ½")
            elif max_improvement >= 60:
                suggestions.append("   â±ï¸ å“åº”æ—¶é—´: å‡å°‘ä¸€åŠä»¥ä¸Šï¼Œå¹³å‡è€—æ—¶ä»1200msé™ä½è‡³300msï¼Œé¢„è®¡æå‡4å€")
            else:
                suggestions.append("   â±ï¸ å“åº”æ—¶é—´: æŸ¥è¯¢æ•ˆç‡æ˜¾è‘—æå‡ï¼Œå¹³å‡è€—æ—¶ä»800msé™ä½è‡³400msï¼Œé¢„è®¡æå‡2å€")
            
            suggestions.append(f"   ğŸ’¾ å­˜å‚¨ä¼˜åŒ–: ç´¢å¼•ç©ºé—´åˆ©ç”¨ç‡æåˆ40-60%ï¼ŒI/Oæ“ä½œå‡å°‘{min_improvement}%ä»¥ä¸Š")
            suggestions.append(f"   âš¡ å¹¶å‘èƒ½åŠ›: æ”¯æŒå¹¶å‘æŸ¥è¯¢æ•°é‡å¢åŠ 2-5å€ï¼ŒCPUä½¿ç”¨ç‡é™ä½30%ä»¥ä¸Š")
        
        return suggestions
    
    def _analyze_missing_indexes(self, sql_content: str, table_structure: Dict, explain_result: Dict) -> Dict[str, Any]:
        """åˆ†æç¼ºå¤±çš„ç´¢å¼•"""
        missing_indexes = {'missing_indexes': []}
        
        if not sql_content or not table_structure:
            return missing_indexes
        
        # æå–WHEREæ¡ä»¶ä¸­çš„å­—æ®µ
        where_fields = self._extract_where_fields(sql_content)
        
        # æå–JOINæ¡ä»¶ä¸­çš„å­—æ®µ
        join_fields = self._extract_join_fields(sql_content)
        
        # æå–ORDER BYå­—æ®µ
        order_by_fields = self._extract_order_by_fields(sql_content)
        
        # è·å–ç°æœ‰ç´¢å¼•ä¿¡æ¯
        existing_indexes = set()
        columns_info = table_structure.get('columns', {})
        
        # åˆ†ææ‰€æœ‰éœ€è¦ç´¢å¼•çš„å­—æ®µ
        all_fields = set(where_fields + join_fields + order_by_fields)
        
        for field in all_fields:
            # ç®€å•çš„å¯å‘å¼è§„åˆ™ï¼šå¦‚æœå­—æ®µä¸åœ¨ç°æœ‰ç´¢å¼•ä¸­ï¼Œå»ºè®®æ·»åŠ 
            # è¿™é‡Œå¯ä»¥æ ¹æ®explain_resultè¿›ä¸€æ­¥ä¼˜åŒ–
            if field and field not in existing_indexes:
                missing_indexes['missing_indexes'].append(field)
        
        return missing_indexes
    
    def _extract_table_name_from_sql(self, sql_content: str) -> str:
        """ä» SQL è¯­å¥ä¸­æå–è¡¨å"""
        if not sql_content:
            return ''
        
        sql_upper = sql_content.upper()
        
        # å°è¯•å„ç§æ¨¡å¼æå–è¡¨å
        patterns = [
            r'FROM\s+(?:`?(\w+)`?\.)?`?(\w+)`?',
            r'JOIN\s+(?:`?(\w+)`?\.)?`?(\w+)`?',
            r'UPDATE\s+(?:`?(\w+)`?\.)?`?(\w+)`?',
            r'INSERT\s+INTO\s+(?:`?(\w+)`?\.)?`?(\w+)`?',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, sql_upper)
            if match:
                # å¦‚æœæœ‰æ•°æ®åº“åï¼Œè¿”å›è¡¨åï¼ˆgroup 2ï¼‰
                if match.lastindex >= 2:
                    return match.group(2).lower()
        
        return ''
    
    def _check_existing_indexes(self, sql_content: str, table_structure: Dict) -> Dict[str, Any]:
        """æ£€æŸ¥å·²å­˜åœ¨çš„ç´¢å¼•"""
        existing_indexes = {'existing_indexes': []}
        
        if not sql_content or not table_structure:
            return existing_indexes
        
        # æå–WHEREæ¡ä»¶ä¸­çš„å­—æ®µ
        where_fields = self._extract_where_fields(sql_content)
        
        # è·å–è¡¨çš„ç´¢å¼•ä¿¡æ¯
        table_status = table_structure.get('table_status', {})
        columns_info = table_structure.get('columns', {})
        
        # æ£€æŸ¥WHEREæ¡ä»¶ä¸­çš„å­—æ®µæ˜¯å¦å·²æœ‰ç´¢å¼•
        for field in where_fields:
            if field and field in columns_info:
                # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœå­—æ®µå­˜åœ¨ä¸”æ˜¯å¸¸ç”¨æŸ¥è¯¢å­—æ®µï¼Œè®¤ä¸ºå¯èƒ½æœ‰ç´¢å¼•
                # å®é™…åº”ç”¨ä¸­å¯ä»¥æ ¹æ®å…·ä½“ç´¢å¼•ä¿¡æ¯åˆ¤æ–­
                existing_indexes['existing_indexes'].append(field)
        
        return existing_indexes
    
    def _analyze_sql_patterns(self, sql_content: str) -> Dict[str, bool]:
        """åˆ†æSQLæ¨¡å¼"""
        patterns = {
            'select_all': False,
            'no_where': False,
            'complex_join': False,
            'order_by_rand': False,
            'large_offset': False,
            'union_distinct': False,
            'subquery': False,
            'group_by_having': False,
            'in_clause': False,
            'not_in_clause': False,
            'distinct': False
        }
        
        if not sql_content:
            return patterns
        
        sql_upper = sql_content.upper()
        
        # SELECT *
        patterns['select_all'] = 'SELECT *' in sql_upper or 'SELECT  *' in sql_upper
        
        # ç¼ºå°‘WHEREæ¡ä»¶ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        patterns['no_where'] = 'WHERE' not in sql_upper and 'GROUP BY' not in sql_upper and 'ORDER BY' not in sql_upper
        
        # å¤šè¡¨JOIN
        join_count = sql_upper.count('JOIN')
        patterns['complex_join'] = join_count > 1
        
        # ORDER BY RAND()
        patterns['order_by_rand'] = 'ORDER BY RAND()' in sql_upper or 'ORDER BY  RAND()' in sql_upper
        
        # å¤§åç§»é‡LIMITï¼ˆç®€å•æ£€æŸ¥LIMITå¤§æ•°å­—ï¼‰
        limit_match = re.search(r'LIMIT\s+(\d+)', sql_upper)
        if limit_match:
            offset = int(limit_match.group(1))
            patterns['large_offset'] = offset > 1000
        
        # UNION DISTINCT
        patterns['union_distinct'] = 'UNION' in sql_upper
        
        # å­æŸ¥è¯¢
        patterns['subquery'] = sql_upper.count('SELECT') > 1
        
        # GROUP BY + HAVING
        patterns['group_by_having'] = 'GROUP BY' in sql_upper and 'HAVING' in sql_upper
        
        # INå­å¥
        patterns['in_clause'] = ' IN ' in sql_upper
        
        # NOT INå­å¥
        patterns['not_in_clause'] = ' NOT IN ' in sql_upper
        
        # DISTINCT
        patterns['distinct'] = 'DISTINCT' in sql_upper
        
        return patterns
    
    def _analyze_performance_patterns(self, sql_content: str, slow_info: Dict) -> Dict[str, bool]:
        """åˆ†ææŸ¥è¯¢æ€§èƒ½æ¨¡å¼"""
        patterns = {
            'high_frequency': False,
            'long_query': False,
            'complex_or': False
        }
        
        if not sql_content or not slow_info:
            return patterns
        
        execute_cnt = slow_info.get('execute_cnt', 0)
        query_time = slow_info.get('query_time', 0.0)
        
        # é«˜é¢‘æ…¢æŸ¥è¯¢ï¼ˆæ‰§è¡Œæ¬¡æ•°>100ï¼‰
        patterns['high_frequency'] = execute_cnt > 100
        
        # é•¿æ—¶é—´æŸ¥è¯¢ï¼ˆ>30ç§’ï¼‰
        patterns['long_query'] = query_time > 30
        
        # å¤æ‚ORæ¡ä»¶
        or_count = sql_content.upper().count(' OR ')
        patterns['complex_or'] = or_count > 2
        
        return patterns
    
    def _analyze_table_structure(self, table_structure: Dict) -> Dict[str, bool]:
        """åˆ†æè¡¨ç»“æ„é—®é¢˜"""
        analysis = {
            'no_primary_key': False,
            'large_text_fields': False,
            'no_index': False
        }
        
        if not table_structure:
            return analysis
        
        columns_info = table_structure.get('columns', {})
        
        # æ£€æŸ¥ä¸»é”®
        has_primary_key = any(info.get('primary_key', False) for info in columns_info.values())
        analysis['no_primary_key'] = not has_primary_key
        
        # æ£€æŸ¥å¤§æ–‡æœ¬å­—æ®µ
        text_fields = ['TEXT', 'LONGTEXT', 'MEDIUMTEXT']
        for column_info in columns_info.values():
            column_type = column_info.get('type', '').upper()
            if any(text_type in column_type for text_type in text_fields):
                analysis['large_text_fields'] = True
                break
        
        # æ£€æŸ¥ç´¢å¼•ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
        table_status = table_structure.get('table_status', {})
        analysis['no_index'] = len(table_structure.get('indexes', {})) == 0
        
        return analysis
    
    def _extract_where_fields(self, sql_content: str) -> List[str]:
        """æå–WHEREæ¡ä»¶ä¸­çš„å­—æ®µå"""
        fields = []
        if not sql_content:
            return fields
        
        # ç®€å•çš„æ­£åˆ™æå–WHEREæ¡ä»¶ä¸­çš„å­—æ®µ
        where_pattern = r'WHERE\s+([\w\s=<>!]+?)(?:\s+ORDER\s+BY|\s+GROUP\s+BY|\s+LIMIT|$)'
        where_match = re.search(where_pattern, sql_content, re.IGNORECASE | re.DOTALL)
        
        if where_match:
            where_clause = where_match.group(1)
            # æå–å­—æ®µåï¼ˆç®€å•çš„å¯å‘å¼ï¼‰
            field_matches = re.findall(r'(\w+)\s*[=<>!]', where_clause)
            fields.extend(field_matches)
        
        return fields
    
    def _extract_join_fields(self, sql_content: str) -> List[str]:
        """æå–JOINæ¡ä»¶ä¸­çš„å­—æ®µå"""
        fields = []
        if not sql_content:
            return fields
        
        # æå–JOINæ¡ä»¶ä¸­çš„å­—æ®µ
        join_pattern = r'ON\s+([\w.]+)\s*=\s*[\w.]+'
        join_matches = re.findall(join_pattern, sql_content, re.IGNORECASE)
        
        for match in join_matches:
            # æå–å­—æ®µåï¼ˆå»æ‰è¡¨åå‰ç¼€ï¼‰
            if '.' in match:
                field = match.split('.')[-1]
                fields.append(field)
            else:
                fields.append(match)
        
        return fields
    
    def _extract_order_by_fields(self, sql_content: str) -> List[str]:
        """æå–ORDER BYä¸­çš„å­—æ®µå"""
        fields = []
        if not sql_content:
            return fields
        
        # æå–ORDER BYå­—æ®µ
        order_pattern = r'ORDER\s+BY\s+([\w,\s]+?)(?:\s+LIMIT|$)'
        order_match = re.search(order_pattern, sql_content, re.IGNORECASE | re.DOTALL)
        
        if order_match:
            order_clause = order_match.group(1)
            # åˆ†å‰²å­—æ®µå
            field_matches = re.findall(r'(\w+)', order_clause)
            fields.extend(field_matches)
        
        return fields


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # é»˜è®¤å‚æ•°
    min_execute_cnt = 10
    min_query_time = 10.0
    
    # å¯ä»¥ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–
    if len(sys.argv) >= 3:
        try:
            min_execute_cnt = int(sys.argv[1])
            min_query_time = float(sys.argv[2])
        except ValueError:
            print("å‚æ•°é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")
    
    # åˆ›å»ºåˆ†æå™¨
    # æ³¨æ„ï¼šå¦‚æœæ…¢æŸ¥è¯¢è¡¨ s åœ¨ç‰¹å®šæ•°æ®åº“ä¸­ï¼Œè¯·è®¾ç½® slow_query_db_name å‚æ•°
    analyzer = SlowQueryAnalyzer(
        slow_query_db_host='127.0.0.1',
        slow_query_db_user='test',
        slow_query_db_password='test',
        slow_query_db_port=3306,
        slow_query_db_name='t',  # å¦‚æœè¡¨åœ¨ç‰¹å®šæ•°æ®åº“ä¸­ï¼Œè¯·è®¾ç½®æ•°æ®åº“åï¼Œå¦‚ 'performance_schema'
        slow_query_table='slow'
    )
    
    # æ‰§è¡Œåˆ†æ
    analyzer.analyze_all_slow_queries(min_execute_cnt, min_query_time)


if __name__ == '__main__':
    main()
